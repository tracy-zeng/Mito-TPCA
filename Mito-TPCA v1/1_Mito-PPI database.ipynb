{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2670307d-39fa-43f5-b1ca-ff58260afea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import copy\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pylab as pltylab\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from adjustText import adjust_text\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from venn import venn\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "import matplotlib.image as mpimg\n",
    "import venn\n",
    "import multiprocessing as mp\n",
    "from scipy.stats import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import pylab as py \n",
    "warnings.filterwarnings('ignore')\n",
    "from Slim_TPCA import Slim_TPCA\n",
    "from scipy.stats import gaussian_kde\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83107f1e-0ffd-4e69-9544-f0f1bfbac467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: Error output settings\n",
    "def logging_output(err_string, mode_string) :\n",
    "    if mode_string == 'debug' :\n",
    "        logging.debug('this is the '+str(err_string))\n",
    "    if mode_string == 'info' :\n",
    "        logging.info('this is the '+str(err_string))\n",
    "    if mode_string == 'warning' :\n",
    "        logging.warning('this is the '+str(err_string))\n",
    "    if mode_string == 'error' :\n",
    "        logging.error('this is the '+str(err_string))\n",
    "    if mode_string == 'critical' :\n",
    "        logging.critical('this is the '+str(err_string))\n",
    "    if mode_string == 'test' :\n",
    "        logging.critical('this is for test')\n",
    "\n",
    "def roc_simple_plot(positive_data_list, negtive_data_list, roc_custom_random_number, savefile): \n",
    "    roc_score=copy.deepcopy(positive_data_list)\n",
    "    roc_label=[1 for unit in positive_data_list]\n",
    "    negtive_data_list=[unit*1 for unit in negtive_data_list]\n",
    "    roc_score += list(1 * np.array(random.sample(negtive_data_list,roc_custom_random_number))) \n",
    "    roc_label += [0] * roc_custom_random_number\n",
    "    #print(roc_score)\n",
    "    #roc_curve:Plot Receiver operating characteristic (ROC) curve\n",
    "    fpr,tpr,threshold = roc_curve(roc_label, roc_score, pos_label = 1)\n",
    "    #auc:Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.2)\n",
    "    plt.plot(fpr, tpr, label='AUC={}'.format(round(roc_auc,4)))\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc=4, fontsize=12)\n",
    "    plt.title('ROC Curve', fontsize=14)\n",
    "    plt.gcf().set_facecolor('white')\n",
    "    if savefile != '' :\n",
    "        plt.savefig(savefile,dpi=600)\n",
    "    return roc_auc\n",
    "\n",
    "# Analyze the results of train\n",
    "def roc_simple(positive_data_list, negtive_data_list, roc_custom_random_number): \n",
    "    roc_score=copy.deepcopy(positive_data_list)\n",
    "    roc_label=[1 for unit in positive_data_list]\n",
    "    negtive_data_list=[unit*(-1) for unit in negtive_data_list]\n",
    "    roc_score += list(-1 * np.array(random.sample(negtive_data_list,roc_custom_random_number))) \n",
    "    roc_label += [0] * roc_custom_random_number\n",
    "    #print(roc_score)\n",
    "    #roc_curve:Plot Receiver operating characteristic (ROC) curve\n",
    "    fpr,tpr,threshold = roc_curve(roc_label, roc_score, pos_label = 1)\n",
    "    #auc:Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "# sub-function: one_list mode is using a given column name, customize a pair of protein pairs\n",
    "# sub-function: two_list mode is using a given column name, customize a pair of protein pairs\n",
    "# Use two columns to obtain paired proteins or states. Remember, here the two \n",
    "# columns of elements must not intersect at all\n",
    "# sub-function: status_list is customize protein state pairs based on the given merge(column name is protein+_+state) table\n",
    "# The paired list is mock:infect1; newline mock:infect2, etc.\n",
    "def pair_table_make(target_list, pair_method) :\n",
    "    if pair_method=='one_list' :\n",
    "        pair_protein_list = []\n",
    "        pair_protein_A_list = []\n",
    "        pair_protein_B_list = []\n",
    "        pair_table1_list = [[1 for j in range(len(target_list))] for i in range(len(target_list))]\n",
    "        np_triangles_list = np.triu(pair_table1_list, 1).tolist()\n",
    "        for i in range(len(np_triangles_list)) :\n",
    "            for j in range(len(np_triangles_list[i])) :\n",
    "                if np_triangles_list[i][j] == 1 :\n",
    "                    #pair_protein_list.append([target_list[i], target_list[j]])\n",
    "                    pair_protein_A_list.append(target_list[i])\n",
    "                    pair_protein_B_list.append(target_list[j])\n",
    "        pair_protein_list.append(pair_protein_A_list)\n",
    "        pair_protein_list.append(pair_protein_B_list)\n",
    "        return pair_protein_list\n",
    "    if pair_method=='two_list' :\n",
    "        pair_protein_list = []\n",
    "        pair_protein_A_list = []\n",
    "        pair_protein_B_list = []\n",
    "        for i in range(len(target_list[0])) :\n",
    "            for j in range(len(target_list[1])) :\n",
    "                pair_protein_A_list.append(target_list[0][i])\n",
    "                pair_protein_B_list.append(target_list[1][j])\n",
    "        pair_protein_list.append(pair_protein_A_list)\n",
    "        pair_protein_list.append(pair_protein_B_list)\n",
    "        return pair_protein_list\n",
    "    if pair_method=='status_list' :\n",
    "        merge_table=target_list[0]\n",
    "        pair_suffix_list=target_list[1]\n",
    "        logging_output('Function: status_pair_table_maker start','debug')\n",
    "        list_pro = merge_table.iloc[:,0].tolist()\n",
    "        status1_pro_list=[]\n",
    "        status2_pro_list=[]\n",
    "        pro_list=[]\n",
    "        for pro_status in list_pro :\n",
    "            if '_' in pro_status :\n",
    "                pro_status_list=pro_status.split('_')\n",
    "                pro_list.append(pro_status_list[0])\n",
    "            else :\n",
    "                pro_list.append(pro_status)\n",
    "        pro_list_unique=list(set(pro_list))\n",
    "        for pro_unit in pro_list_unique :\n",
    "            for pair_suffix_list_index in range(len(pair_suffix_list[0])) :\n",
    "                status1_pro_list.append(pro_unit+pair_suffix_list[0][pair_suffix_list_index])\n",
    "                status2_pro_list.append(pro_unit+pair_suffix_list[1][pair_suffix_list_index])\n",
    "        pair_table=pd.DataFrame({'Status A':status1_pro_list, 'Status B':status2_pro_list})\n",
    "        pair_table_found = pair_table[np.array([pair_table.iloc[:,0][i] in list_pro for i in range(len(pair_table))]) &\n",
    "                                      np.array([pair_table.iloc[:,1][i] in list_pro for i in range(len(pair_table))])].reset_index(drop=True)\n",
    "        return pair_table_found\n",
    "\n",
    "\n",
    "\n",
    "# Use two columns of data for comparison and add the comparison values to the first table\n",
    "def pd_col_exchange_for_merge(pd_table1, alig_table1_col1_name, alig_table1_col2_name) :\n",
    "    alig_table1_col1_list=pd_table1[alig_table1_col1_name].tolist()\n",
    "    alig_table1_col2_list=pd_table1[alig_table1_col2_name].tolist()\n",
    "    new_col1_list=[]\n",
    "    new_col2_list=[]\n",
    "    for i in range(len(alig_table1_col1_list)) :\n",
    "        if alig_table1_col1_list[i] < alig_table1_col2_list[i] :\n",
    "            new_col1_list.append(alig_table1_col1_list[i])\n",
    "            new_col2_list.append(alig_table1_col2_list[i])\n",
    "        else :\n",
    "            new_col1_list.append(alig_table1_col2_list[i])\n",
    "            new_col2_list.append(alig_table1_col1_list[i])\n",
    "    pd_new_table1=copy.deepcopy(pd_table1)\n",
    "    pd_new_table1[alig_table1_col1_name]=new_col1_list\n",
    "    pd_new_table1[alig_table1_col2_name]=new_col2_list\n",
    "    return pd_new_table1\n",
    "\n",
    "\n",
    "# Use two columns of data for comparison and add the comparison values to the first table\n",
    "def pd_col_exchange_for_merge_simple(cmp_name) :\n",
    "    colname_list=cmp_name.split('_vs_')\n",
    "    colname1=colname_list[0]\n",
    "    colname2=colname_list[1]\n",
    "    new_colname1=''\n",
    "    if colname1 < colname2 :\n",
    "        new_colname1=colname2+'_vs_'+colname1\n",
    "    else :\n",
    "        new_colname1=colname1+'_vs_'+colname2\n",
    "    return new_colname1\n",
    "\n",
    "# Median correction according to the column specified by median_list\n",
    "def median_value_correction(table_for_median, median_list) :\n",
    "    result_table=copy.deepcopy(table_for_median)\n",
    "    m_value_array = np.median(result_table.iloc[:,median_list], axis=0)\n",
    "    m_value = np.median(m_value_array)\n",
    "    print(m_value_array);print(m_value)\n",
    "    for i in range(len(median_list)):\n",
    "        result_table.iloc[:,[median_list[i]]] = result_table.iloc[:,[median_list[i]]] / m_value_array[i] * m_value\n",
    "    return result_table\n",
    "\n",
    "\n",
    "\n",
    "def merge_repeat(pd_table, merge_repeat_dict) :\n",
    "    print(merge_repeat_dict['accession_index'])\n",
    "    accession_index=merge_repeat_dict['accession_index']\n",
    "    repeat_list=merge_repeat_dict['repeat_list']\n",
    "    PSM_list=merge_repeat_dict['PSM_list']\n",
    "    PSM_sum_index=merge_repeat_dict['PSM_sum_index']\n",
    "    pd_table_list_1=[pd_table.iloc[:, repeat_unit] for repeat_unit in repeat_list]\n",
    "    print(pd_table_list_1)\n",
    "    pd_table_list_2=[pd_table_list_1[repeat_index].values*pd_table.iloc[:, [PSM_list[repeat_index] for unit in range(pd_table_list_1[repeat_index].shape[1])]].values for repeat_index in range(len(pd_table_list_1))]\n",
    "    print(pd_table_list_2)\n",
    "    pd_table_list_zero=np.zeros(pd_table_list_2[0].shape)\n",
    "    print(pd_table_list_zero)\n",
    "    for unit in pd_table_list_2 : pd_table_list_zero=pd_table_list_zero+unit; print(pd_table_list_zero)\n",
    "    pd_table_list_zero=pd_table_list_zero/pd_table.iloc[:, [PSM_sum_index for unit in range(pd_table_list_zero.shape[1])]]\n",
    "    print(pd_table.iloc[:, [PSM_sum_index for unit in range(pd_table_list_zero.shape[1])]])\n",
    "    print(pd_table_list_zero)\n",
    "    pd_merge_repeat_table=pd.concat([pd_table.iloc[:, [accession_index]], pd_table_list_zero], axis=1)\n",
    "    pd_merge_repeat_table_colname_list=copy.deepcopy([accession_index])\n",
    "    pd_merge_repeat_table_colname_list.extend(repeat_list[0])\n",
    "    pd_merge_repeat_table_colname=[pd_table.columns.tolist()[unit] for unit in pd_merge_repeat_table_colname_list]\n",
    "    pd_merge_repeat_table.columns=pd_merge_repeat_table_colname\n",
    "    return pd_merge_repeat_table\n",
    "\n",
    "\n",
    "\n",
    "# first TPCA function\n",
    "# function: Normalised TPCA curves\n",
    "# Use a column as the denominator and the rest \n",
    "# of the columns as the denominator for the operation\n",
    "# Divided by a column of ref_col, converted to soluble fraction\n",
    "# loc：works on labels in the index.\n",
    "# iloc：works on the positions in the index (so it only takes integers).\n",
    "def preproc(table, ref_col=1):\n",
    "    logging_output(\"------preproc\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    if ref_col == False:\n",
    "        return table\n",
    "    else:\n",
    "        table_clean = copy.copy(table)\n",
    "        for col in table_clean.columns[1:]: \n",
    "            # part 1\n",
    "            soluble_fraction = table_clean.loc[:,col] / table.iloc[:, ref_col]\n",
    "            table_clean.loc[:,col] = soluble_fraction\n",
    "        logging_output(table_clean,'debug')\n",
    "        logging_output(\"------preproc:end\",'debug')\n",
    "        return table_clean\n",
    "\n",
    "\n",
    "# function: Calculating distances of paired matrices, and obtaining the matrix\n",
    "# Generate a two-dimensional matrix where the values at \n",
    "# the (i,j) positions represent the distance between the \n",
    "# protein represented in row i and the protein represented in column j\n",
    "# ---------sub function---------\n",
    "# preproc: preproc return dist table\n",
    "# distance.cdist: Compute distance between each pair of the two collections of inputs.\n",
    "# round: Retain a few decimal places\n",
    "# ---------sub function---------\n",
    "def dist(table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------dist\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    table_clean = preproc(table, ref_col)\n",
    "    table_values = tuple(table.iloc[:,1:].values)\n",
    "    dist_matrix = pd.DataFrame(distance.cdist(table_values, table_values, metric=method), index=table_clean.iloc[:,0], columns = table_clean.iloc[:,0])\n",
    "    dist_matrix.index.name = ''\n",
    "    dist_matrix.columns.name = ''\n",
    "    logging_output(round(dist_matrix,6),'debug')\n",
    "    logging_output(\"------dist:end\",'debug')\n",
    "    return round(dist_matrix,6)\n",
    "\n",
    "\n",
    "# function: find the presence of paired proteins in pair_table table by search table\n",
    "# reset_index: It is to remove the original column index and reset the index\n",
    "# explain the line in part 1 in the bellow\n",
    "# CMD1: 'a' in ['a','b','c'] for i in range(len(['a','b','c']))\n",
    "# keypoint is the i\n",
    "# RESULT1: [True, False, True]\n",
    "# CMD1: np.array([True, False, True]) & np.array([True, True, True])\n",
    "# RESULT1: array([ True, False,  True])\n",
    "def pair_found(table, pair_table, ref_col=1):\n",
    "    logging_output(\"------pair_found\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(pair_table,'debug')\n",
    "    table_clean = preproc(table, ref_col)\n",
    "    list_pro = list(table_clean.iloc[:,0])\n",
    "    # part 1\n",
    "    pair_table_found = pair_table[np.array([pair_table.iloc[:,0][i] in list_pro for i in range(len(pair_table))]) &\n",
    "                                  np.array([pair_table.iloc[:,1][i] in list_pro for i in range(len(pair_table))])].reset_index(drop=True)\n",
    "    logging_output(pair_table_found,'debug')\n",
    "    logging_output(\"------pair_found:end\",'debug')\n",
    "    return pair_table_found\n",
    "\n",
    "\n",
    "def roc(table, pair_table, negtive_number, ref_col=1, method='cityblock'): # 计算ROC的TPR, FPR, AUC\n",
    "    pair_table_found = pair_found(table, pair_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    roc_label, roc_score = [], [];i=0\n",
    "    for i in range(len(pair_table_found)): # 寻找positive的数�\n",
    "        pro_a, pro_b = pair_table_found.iloc[i,0], pair_table_found.iloc[i,1]\n",
    "        roc_score.append(-1 * dist_matrix.loc[pro_a,pro_b])\n",
    "        roc_label.append(1);i=i+1\n",
    "        dist_matrix.loc[pro_a, pro_b], dist_matrix.loc[pro_b, pro_a] = 0,0\n",
    "    neg_values = np.triu(dist_matrix, k=0).flatten()\n",
    "    neg_values = neg_values[neg_values!=0]\n",
    "    random.seed(42)\n",
    "    if negtive_number > len(neg_values) :\n",
    "        negtive_number = len(neg_values)\n",
    "    roc_score += list(-1 * np.array(random.sample(list(neg_values),negtive_number))) # 混入10,000个negative的数�\n",
    "    roc_label += [0] * negtive_number\n",
    "    fpr,tpr,threshold = roc_curve(roc_label, roc_score, pos_label = 1)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    return fpr, tpr, round(roc_auc,4)\n",
    "\n",
    "\n",
    "def roc_score_label(table, pair_table, negtive_number, ref_col=1, method='cityblock'): # 计算ROC的TPR, FPR, AUC\n",
    "    pair_table_found = pair_found(table, pair_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    roc_label, roc_score = [], [];i=0\n",
    "    roc_real_label=[]\n",
    "    for i in range(len(pair_table_found)): # 寻找positive的数�\n",
    "        pro_a, pro_b = pair_table_found.iloc[i,0], pair_table_found.iloc[i,1]\n",
    "        roc_score.append(-1 * dist_matrix.loc[pro_a,pro_b])\n",
    "        roc_label.append(1);i=i+1;print(i)\n",
    "        dist_matrix.loc[pro_a, pro_b], dist_matrix.loc[pro_b, pro_a] = 0,0\n",
    "        roc_real_label.append(pro_a+'_vs_'+pro_b)\n",
    "    neg_values = np.triu(dist_matrix, k=0).flatten()\n",
    "    neg_values = neg_values[neg_values!=0]\n",
    "    random.seed(42)\n",
    "    if negtive_number > len(neg_values) :\n",
    "        negtive_number = len(neg_values)\n",
    "    # roc_score += list(-1 * np.array(random.sample(list(neg_values),negtive_number))) # 混入10,000个negative的数�\n",
    "    # roc_label += [0] * negtive_number\n",
    "    # fpr,tpr,threshold = roc_curve(roc_label, roc_score, pos_label = 1)\n",
    "    # roc_auc = auc(fpr,tpr)\n",
    "    return roc_label, roc_score, roc_real_label\n",
    "\n",
    "\n",
    "def roc_plot(table, pair_table, negtive_number, ref_col=1, method='cityblock'): # 根据上一步得到的FPR、TPR来画ROC画图函数\n",
    "    fpr, tpr, roc_auc = roc(table, pair_table, negtive_number, ref_col, method)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.2)\n",
    "    plt.plot(fpr, tpr, label='AUC={}'.format(roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc=4, fontsize=12)\n",
    "    plt.title('ROC Curve', fontsize=14)\n",
    "    return roc_auc\n",
    "\n",
    "\n",
    "# function: Convert the distance matrix of pairs of proteins into a one-dimensional list and place it in a panda table\n",
    "# Calculate the distance of protein pairs\n",
    "#Euclidean distance calculation with elements common to paired matrices\n",
    "#The following comments are the format of the returned data\n",
    "#  Protein A Protein B  No. of Publications      Dist\n",
    "#0    P40337    Q16665                   41  5.185614\n",
    "def pair_dist(table, pair_table, ref_col=1, method='cityblock'): \n",
    "    logging_output(\"------pair_dist\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(pair_table,'debug')\n",
    "    pair_table_found = pair_found(table, pair_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    l_dist = []\n",
    "    for i in range(len(pair_table_found)):\n",
    "        pro_a, pro_b = pair_table_found.iloc[i,0], pair_table_found.iloc[i,1]\n",
    "        d = dist_matrix.loc[pro_a, pro_b]\n",
    "        l_dist.append(d)\n",
    "    pair_table_found['Dist'] = l_dist\n",
    "    logging_output(pair_table_found,'debug')\n",
    "    logging_output(\"------pair_dist:end\",'debug')\n",
    "    return pair_table_found\n",
    "\n",
    "\n",
    "def pair_dist_simple(pair_table_found, dist_matrix): \n",
    "    logging_output(\"------pair_dist_simple\",'debug')\n",
    "    logging_output(pair_table_found,'debug')\n",
    "    logging_output(dist_matrix,'debug')\n",
    "    l_dist = []\n",
    "    for i in range(len(pair_table_found)):\n",
    "        pro_a, pro_b = pair_table_found.iloc[i,0], pair_table_found.iloc[i,1]\n",
    "        d = dist_matrix.loc[pro_a, pro_b]\n",
    "        l_dist.append(d)\n",
    "    pair_table_found['Dist'] = l_dist\n",
    "    logging_output(pair_table_found,'debug')\n",
    "    logging_output(\"------pair_dist_simple:end\",'debug')\n",
    "    return pair_table_found\n",
    "\n",
    "\n",
    "# function: The significance of the distance of each paired protein was \n",
    "# calculated using the data of all paired protein distances as input data.\n",
    "# The probability (p-value) is calculated by randomly selecting the overall \n",
    "# data and calculating the ranking of that data in this data.\n",
    "def pair_signature(pair_signature_random_number, table, pair_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------pair_signature\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(pair_table,'debug')\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    pair_table_found = pair_dist(table, pair_table, ref_col, method)\n",
    "    dist_values = np.triu(dist_matrix, k = 0).flatten()\n",
    "    dist_values = list(dist_values[dist_values != 0])\n",
    "    random.seed(42)\n",
    "    if pair_signature_random_number > len(dist_values) :\n",
    "        pair_signature_random_number = len(dist_values)\n",
    "    dist_values = random.sample(dist_values, pair_signature_random_number)\n",
    "    l_sig = []\n",
    "    for i in range(len(pair_table_found)):\n",
    "        #pro_a, pro_b have no means???\n",
    "        pro_a, pro_b = pair_table_found.iloc[i,0], pair_table_found.iloc[i,1]\n",
    "        d = pair_table_found['Dist'][i]\n",
    "        l_sig.append(np.sum(dist_values < d) / pair_signature_random_number)\n",
    "        print(\"------pair_signature: cycle\"+str(len(pair_table_found)))\n",
    "        print(\"------pair_signature: cycle\"+str(i))\n",
    "    pair_table_found['TPCA_Sig'] = l_sig\n",
    "    # print(pair_table_found)\n",
    "    print(\"------pair_signature:end\")\n",
    "    return pair_table_found\n",
    "\n",
    "\n",
    "# function: Find the eligible protein complexes from the table, \n",
    "#---------where the protein complex exists as long as the protein in the protein complex appears in the table again\n",
    "# ------Looking for complexes for subsequent analysis, the criterion was that at least three proteins were identified to\n",
    "#Returns the number of proteins in the specified complex\n",
    "#reset_index: Reset the index of the DataFrame, and use the default one instead\n",
    "def complex_found(table, complex_table, ref_col=1): \n",
    "    logging_output(\"------complex_found\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    table_clean = preproc(table, ref_col)\n",
    "    complex_out = complex_table[complex_table['Organism']=='Human'].reset_index(drop=True)\n",
    "    sub_found, num_sub_found = [], []\n",
    "    for i in range(len(complex_out)):\n",
    "        l_sub = complex_out['subunits(UniProt IDs)'][i].split(';')\n",
    "        l_found = [sub for sub in l_sub if sub in list(table_clean.iloc[:,0])]\n",
    "        sub_found.append(';'.join(l_found))\n",
    "        num_sub_found.append(len(l_found))\n",
    "        logging_output(\"------complex_found: cycle\"+str(len(complex_out)),'debug')\n",
    "        logging_output(\"------complex_found: cycle\"+str(i),'debug')\n",
    "    complex_out['Subunit_Found'] = sub_found\n",
    "    complex_out['No_Subunit_Found'] = num_sub_found # of proteins identified in the complex\n",
    "    complex_out = complex_out[complex_out['No_Subunit_Found']>1].reset_index(drop=True)\n",
    "    logging_output(complex_out,'debug')\n",
    "    logging_output(\"------complex_found:end\",'debug')\n",
    "    return complex_out\n",
    "\n",
    "\n",
    "# function: Detecting the significance of a protein complex in terms of the number of proteins in \n",
    "# it (a certain number of randomly generated protein complexes with the same number of proteins is used as input data)\n",
    "# Return the mean and standard deviation of individual protein complexes\n",
    "# ComplexID ComplexName ...  Avg_Dist Avg_Dist_Derived\n",
    "# 3 4 Multisubunit ACTR coactivator complex ... 0.830157 0.546401\n",
    "# part 1: Although it will be duplicated,\n",
    "# the averaging algorithm avoids the impact of duplication\n",
    "def complex_dist(table, complex_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------complex_dist\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    complex_table_found = complex_found(table, complex_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    l_dist, l_dist_derived = [], []\n",
    "    for i in range(len(complex_table_found)):\n",
    "        l_sub = complex_table_found['Subunit_Found'][i].split(';')\n",
    "        sub_matrix = dist_matrix.loc[l_sub, l_sub]\n",
    "        # part 1\n",
    "        avg_dist = np.nanmean(sub_matrix.replace(0, np.nan))\n",
    "        l_dist.append(avg_dist)\n",
    "        l_dist_derived.append(1/(1+avg_dist))\n",
    "        logging_output(\"complex_dist:cycle\",'debug')\n",
    "        logging_output(l_sub,'debug')\n",
    "        logging_output(sub_matrix,'debug')\n",
    "        logging_output(\"complex_dist:cycle_end\",'debug')\n",
    "    complex_table_found['Avg_Dist'] = l_dist\n",
    "    complex_table_found['Avg_Dist_Derived'] = l_dist_derived\n",
    "    logging_output(complex_table_found,'debug')\n",
    "    logging_output(\"------complex_dist:end\",'debug')\n",
    "    return complex_table_found\n",
    "\n",
    "\n",
    "# function: Calculating the significance of protein complexes\n",
    "# part 1: Calculate the mean and standard deviation of the overall distance matrix\n",
    "# part 2: #with n and avg_dist calculated in pairs of matrices as parameters.\n",
    "# The mean and variance calculated from the distance matrix of all proteins were used as\n",
    "# parameters to establish the parameters of the normal distribution z\n",
    "# Then obtain the probability that the normal distribution is greater than 0\n",
    "# part 3: Probability of being greater than 0 in a normal distribution\n",
    "def complex_signature_normal(table, complex_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------complex_signature_normal\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    complex_table_found = complex_dist(table, complex_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    mu, sig = np.nanmean(dist_matrix.replace(0, np.nan)), np.nanstd(dist_matrix.replace(0, np.nan)) \n",
    "    l_sig = []\n",
    "    # part 2\n",
    "    for i in range(len(complex_table_found)):\n",
    "        n = complex_table_found['No_Subunit_Found'][i]\n",
    "        avg_dist = complex_table_found['Avg_Dist'][i]\n",
    "        z = (avg_dist - mu) * (n ** (1/2)) / sig\n",
    "        # part 3\n",
    "        l_sig.append(1 - norm.sf(z))\n",
    "        logging_output(\"------complex_signature_normal: cycle\"+str(len(complex_table_found)),'debug')\n",
    "        logging_output(\"------complex_signature_normal: cycle\"+str(i),'debug')\n",
    "    complex_table_found['TPCA_Sig'] = l_sig\n",
    "    logging_output(complex_table_found,'debug')\n",
    "    logging_output(\"------complex_signature_normal:end\",'debug')\n",
    "    return complex_table_found\n",
    "\n",
    "\n",
    "# function: 10,000 protein complexes randomly generated based on the number \n",
    "# of proteins contained in the protein complex.\n",
    "# (2 proteins, 3 proteins, 4 proteins)\n",
    "# Randomly calculate the distance of 10,000 proteins pairs by num is 1,2,3,4 and so on\n",
    "# part 1: Indexed by the number of proteomes forming complexes\n",
    "# The average of the distance matrix of these n groups of proteins \n",
    "# (n indicates the number of proteins in the protein complex)\n",
    "# was examined separately\n",
    "# The above content is randomly sampled 10,000 times,\n",
    "# and the results are added to the corresponding list in turn\n",
    "# Returns a random list for the specified number of proteins\n",
    "# part 2: numpy.nanmean() function can be used to calculate \n",
    "# the average of the array ignoring NaN values\n",
    "def random_10k(random_10k_number, table, complex_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------random_10k\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    l_n = list(set(complex_found(table, complex_table, ref_col)['No_Subunit_Found']))\n",
    "    l_n.sort()\n",
    "    dic_out = {}\n",
    "    pairs_dist_table = dist(table, ref_col, method)\n",
    "    # part 1\n",
    "    for num in l_n:\n",
    "        l_10k_dist = []\n",
    "        random.seed(42)\n",
    "        if num > len(list(pairs_dist_table.index)) :\n",
    "            num = len(list(pairs_dist_table.index))\n",
    "        for i in range(random_10k_number):\n",
    "            random_proteins = random.sample(list(pairs_dist_table.index), num)\n",
    "            random_sub_table = pairs_dist_table.loc[random_proteins, random_proteins].replace(0, np.nan)\n",
    "            # part 2\n",
    "            l_10k_dist.append(np.nanmean(random_sub_table))\n",
    "        dic_out[num] = l_10k_dist\n",
    "        logging_output(\"------random_10k: cycle\"+str(len(l_n)),'debug')\n",
    "        logging_output(\"------random_10k: cycle\"+str(i),'debug')\n",
    "    logging_output(dic_out,'debug')\n",
    "    logging_output(\"------random_10k:end\",'debug')\n",
    "    return dic_out\n",
    "\n",
    "\n",
    "# function: Calculate the empirical P-value, Z-score of the complex\n",
    "# Calculate the empirical P-value, Z-score of the complex\n",
    "# return the number of random_table. the number indicates the number of protein in complex\n",
    "# return complex_table_found have the col of TPCA_Sig_P-value and TPCA_Sig_Z-score\n",
    "# part 1: Z-Score\n",
    "# Compute the z score of each value in the sample, relative to the sample mean and standard deviation\n",
    "# ????Why only the last value is returned\n",
    "# in there stats.zscore(l_random10000_derived)[-1] is avg_dist_derived\n",
    "# Put the value avg_dist_derived into all variables for normalization,then pick it\n",
    "def complex_signature_sample(random_10k_number, complex_signature_sample_random_number, table, complex_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------complex_signature_sample\",'debug')\n",
    "    logging_output(table,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    complex_table_found = complex_dist(table, complex_table, ref_col)\n",
    "    dic_random = random_10k(random_10k_number, table, complex_table, ref_col, method)\n",
    "    p_value, z_score = [], []\n",
    "    for i in range(len(complex_table_found)):\n",
    "        n = complex_table_found['No_Subunit_Found'][i]\n",
    "        avg_dist = complex_table_found['Avg_Dist'][i]\n",
    "        avg_dist_derived = complex_table_found['Avg_Dist_Derived'][i]\n",
    "        l_random10000 = dic_random[n]\n",
    "        #why add avg_dist_derived here\n",
    "        l_random10000_derived = list(1/(1+np.array(l_random10000))) + [avg_dist_derived]\n",
    "        p_value.append(np.sum(np.array(l_random10000)<avg_dist)/complex_signature_sample_random_number)\n",
    "        # part 1\n",
    "        z_score.append(stats.zscore(l_random10000_derived)[-1])\n",
    "        logging_output(\"complex_signature_sample,cycle\",'debug')\n",
    "        logging_output(l_random10000_derived,'debug')\n",
    "        logging_output(stats.zscore(l_random10000_derived),'debug')\n",
    "        logging_output(stats.zscore(l_random10000_derived)[-1],'debug')\n",
    "        logging_output(\"complex_signature_sample,cycle:end\",'debug')\n",
    "    complex_table_found['TPCA_Sig_P-value'] = p_value\n",
    "    complex_table_found['TPCA_Sig_Z-score'] = z_score\n",
    "    random_table = pd.DataFrame(dic_random)\n",
    "    logging_output(random_table,'debug')\n",
    "    logging_output(complex_table_found,'debug')\n",
    "    logging_output(\"------complex_signature_sample:end\",'debug')\n",
    "    return random_table, complex_table_found\n",
    "\n",
    "\n",
    "# function: Compare two lists with each other by their respective \n",
    "# -----indexes and return the result of the comparison\n",
    "# The index rows shared by the two matrices are taken out to form two matrices respectively\n",
    "# part 1: set: Create an unordered set of unduplicated elements\n",
    "# Filter elements that are contained in both lists\n",
    "# part 2: Merge DataFrame or named Series objects with a database-style join\n",
    "# on: The name of the column or index level to join.\n",
    "# Must be found in the left and right DataFrame objects\n",
    "# part 3: reset pandas index\n",
    "def align(table_1, table_2, ref_col=1):\n",
    "    logging_output(\"------align\",'debug')\n",
    "    logging_output(table_1,'debug')\n",
    "    logging_output(table_2,'debug')\n",
    "    table_1_align = preproc(table_1, ref_col)\n",
    "    table_2_align = preproc(table_2, ref_col)\n",
    "    # part 1\n",
    "    list_pro = list(set(table_1_align.iloc[:,0]) & set(table_2_align.iloc[:,0]))\n",
    "    # part 2\n",
    "    table_1_align = pd.merge(table_1_align, pd.DataFrame({table_1_align.columns[0]:list_pro}), on=table_1.columns[0])\n",
    "    table_2_align = pd.merge(table_2_align, pd.DataFrame({table_2_align.columns[0]:list_pro}), on=table_2.columns[0])\n",
    "    # part 3\n",
    "    table_1_align = table_1_align.sort_values(by=table_1_align.columns[0]).reset_index(drop=True)\n",
    "    table_2_align = table_2_align.sort_values(by=table_2_align.columns[0]).reset_index(drop=True)\n",
    "    logging_output(table_1_align.columns[0],'debug')\n",
    "    logging_output(list_pro,'debug')\n",
    "    logging_output(pd.DataFrame({table_1_align.columns[0]:list_pro}),'debug')\n",
    "    logging_output(table_1_align,'debug')\n",
    "    logging_output(table_2_align,'debug')\n",
    "    logging_output(\"------align:end\",'debug')\n",
    "    return table_1_align, table_2_align\n",
    "\n",
    "\n",
    "# function: Calculate the difference in distance between the paired proteins in the two data sets\n",
    "#----No call to\n",
    "def dynamic_pair(dynamic_pair_random_number, table_1, table_2, pair_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------dynamic_pair\",'debug')\n",
    "    logging_output(table_1,'debug')\n",
    "    logging_output(table_2,'debug')\n",
    "    logging_output(pair_table,'debug')\n",
    "    table_1_align, table_2_align = align(table_1, table_2, ref_col)\n",
    "    pair_dist_1 = pair_dist(table_1_align, pair_table, ref_col, method)\n",
    "    pair_dist_2 = pair_dist(table_2_align, pair_table, ref_col, method)\n",
    "    pair_dist_change = pd.merge(pair_dist_1, pair_dist_2, on=list(pair_dist_1.columns[:-1]), suffixes=('_1','_2'))\n",
    "    pair_dist_change['Dist_change'] = pair_dist_change['Dist_1'] - pair_dist_change['Dist_2']\n",
    "    dist_matrix_1 = dist(table_1_align, ref_col)\n",
    "    dist_matrix_2 = dist(table_2_align, ref_col)\n",
    "    dist_matrix_dynamic = dist_matrix_1 - dist_matrix_2\n",
    "    dist_values_dynamic = np.triu(dist_matrix_dynamic, k=0).flatten()\n",
    "    random.seed(42)\n",
    "    if dynamic_pair_random_number > len(list(dist_values_dynamic[dist_values_dynamic!=0])) :\n",
    "        dynamic_pair_random_number = len(list(dist_values_dynamic[dist_values_dynamic!=0]))\n",
    "    print('list(dist_values_dynamic[dist_values_dynamic!=0]) is '+str(len(list(dist_values_dynamic[dist_values_dynamic!=0]))))\n",
    "    print('random number is '+str(dynamic_pair_random_number))\n",
    "    dist_values_dynamic = random.sample(list(dist_values_dynamic[dist_values_dynamic!=0]), dynamic_pair_random_number)\n",
    "    dynamic_p = []\n",
    "    for i in range(len(pair_dist_change)):\n",
    "        dist_change = pair_dist_change['Dist_change'][i]\n",
    "        dynamic_p.append(np.sum(dist_values_dynamic > dist_change) / dynamic_pair_random_number)\n",
    "    pair_dist_change['Dynamic_Sig'] = dynamic_p\n",
    "    logging_output(pair_dist_change,'debug')\n",
    "    logging_output(\"------dynamic_pair:end\",'debug')\n",
    "    return pair_dist_change\n",
    "\n",
    "\n",
    "# function: Calculate the P and Z values for the protein complex in the two data sets\n",
    "# part 1: last four columns names by suffixes(\"-1\",\"-2\")\n",
    "# part 2: Distance matrix of the control group\n",
    "# The distance matrix of the experimental group\n",
    "# Difference matrix: the difference of two distance matrices\n",
    "# The numpy.nanstd() function calculates the standard \n",
    "# deviation along the specified axis while ignoring NaN.\n",
    "# Calculate the mean and standard deviation of the difference matrix\n",
    "def dynamic_complex_normal(table_1, table_2, complex_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------dynamic_complex_normal\",'debug')\n",
    "    logging_output(table_1,'debug')\n",
    "    logging_output(table_2,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    # part 1\n",
    "    table_1_align, table_2_align = align(table_1, table_2, ref_col)\n",
    "    complex_dist_1 = complex_dist(table_1_align, complex_table, ref_col, method)\n",
    "    complex_dist_2 = complex_dist(table_2_align, complex_table, ref_col, method)\n",
    "    complex_dist_change = pd.merge(complex_dist_1, complex_dist_2, on=list(complex_dist_1.columns[:-2]), suffixes=('_1','_2'))\n",
    "    complex_dist_change['Avg_Dist_change'] = complex_dist_change['Avg_Dist_1'] - complex_dist_change['Avg_Dist_2']\n",
    "    complex_dist_change['Avg_Dist_Derived_change'] = complex_dist_change['Avg_Dist_Derived_1'] - complex_dist_change['Avg_Dist_Derived_2']\n",
    "    logging_output(\"dynamic_complex_normal:part1\",'debug')\n",
    "    logging_output(complex_dist_1,'debug')\n",
    "    logging_output(complex_dist_2,'debug')\n",
    "    logging_output(complex_dist_change,'debug')\n",
    "    logging_output(complex_dist_1.columns[:-1],'debug')\n",
    "    logging_output(\"dynamic_complex_normal:part2\",'debug')\n",
    "    # part 2\n",
    "    dist_matrix_1 = dist(table_1_align, ref_col)\n",
    "    dist_matrix_2 = dist(table_2_align, ref_col)\n",
    "    dist_matrix_dynamic = dist_matrix_1 - dist_matrix_2\n",
    "    mu, sig = np.nanmean(dist_matrix_dynamic.replace(0, np.nan)), np.nanstd(dist_matrix_dynamic.replace(0, np.nan))\n",
    "    dynamic_p = []\n",
    "    dynamic_z = []\n",
    "    for i in range(len(complex_dist_change)):\n",
    "        n = complex_dist_change['No_Subunit_Found'][i]\n",
    "        avg_dist_change = complex_dist_change['Avg_Dist_change'][i]\n",
    "        z = (avg_dist_change - mu) * (n ** (1/2)) / sig\n",
    "        dynamic_z.append(z)\n",
    "        dynamic_p.append(norm.sf(z)) # Calculate dynamic P-values\n",
    "        logging_output(\"------dynamic_complex_normal: cycle\"+str(len(complex_dist_change)),'debug')\n",
    "        logging_output(\"------dynamic_complex_normal: cycle\"+str(i),'debug')\n",
    "    complex_dist_change['Dynamic_Z'] = dynamic_z\n",
    "    complex_dist_change['Dynamic_P'] = dynamic_p\n",
    "    logging_output(complex_dist_change,'debug')\n",
    "    logging_output(\"------dynamic_complex_normal:end\",'debug')\n",
    "    return complex_dist_change\n",
    "\n",
    "\n",
    "# function: Calculate the P and Z values for the protein complex in the two data sets\n",
    "# part 1: complex_dist_change['Avg_Dist_Derived_change'] = 1 / (1+complex_dist_change['Avg_Dist_change'])\n",
    "# part 2: ???two random table can subtraction?\n",
    "# part 3: random_table_derived = 1 / (1+random_table)\n",
    "# part 4: the percent point function ppf\n",
    "def dynamic_complex_sample(dynamic_complex_sample_random_numbe, table_1, table_2, complex_table, ref_col=1, method='cityblock'):\n",
    "    logging_output(\"------dynamic_complex_sample\",'debug')\n",
    "    logging_output(table_1,'debug')\n",
    "    logging_output(table_2,'debug')\n",
    "    logging_output(complex_table,'debug')\n",
    "    table_1_align, table_2_align = align(table_1, table_2, ref_col)\n",
    "    complex_dist_1 = complex_dist(table_1_align, complex_table, ref_col, method)\n",
    "    complex_dist_2 = complex_dist(table_2_align, complex_table, ref_col, method)\n",
    "    complex_dist_change = pd.merge(complex_dist_1, complex_dist_2, on=list(complex_dist_1.columns[:-2]), suffixes=('_1','_2'))\n",
    "    complex_dist_change['Avg_Dist_change'] = complex_dist_change['Avg_Dist_1'] - complex_dist_change['Avg_Dist_2']\n",
    "    dic_random_1 = random_10k(table_1, complex_table, ref_col, method)\n",
    "    dic_random_2 = random_10k(table_2, complex_table, ref_col, method)\n",
    "    random_table = pd.DataFrame(dic_random_1) - pd.DataFrame(dic_random_2)\n",
    "    p_value, z_score = [], []\n",
    "    for i in range(len(complex_dist_change)):\n",
    "        n = complex_dist_change['No_Subunit_Found'][i]\n",
    "        avg_dist = complex_dist_change['Avg_Dist_change'][i]\n",
    "        # avg_dist_derived = complex_dist_change['Avg_Dist_Derived_change'][i]\n",
    "        l_random10000 = random_table[n]\n",
    "        # l_random10000_derived = random_table_derived[n] + [avg_dist_derived]\n",
    "        p_value.append(np.sum(np.array(l_random10000)>avg_dist)/dynamic_complex_sample_random_numbe)\n",
    "        # z_score.append(stats.zscore(l_random10000_derived)[-1])\n",
    "        logging_output(\"------dynamic_complex_sample: cycle\"+str(len(complex_dist_change)),'debug')\n",
    "        logging_output(\"------dynamic_complex_sample: cycle\"+str(i),'debug')\n",
    "    complex_dist_change['Dynamic_P-value'] = p_value\n",
    "    # part 4\n",
    "    complex_dist_change['Dynamic_Z-score'] = -1 * norm.ppf(np.array(p_value) / 2)\n",
    "    logging_output(random_table,'debug')\n",
    "    logging_output(complex_dist_change,'debug')\n",
    "    logging_output(\"------dynamic_complex_sample:end\",'debug')\n",
    "    return random_table, complex_dist_change\n",
    "\n",
    "def complex_dist(table, complex_table, ref_col=1, method='cityblock'):\n",
    "    complex_table_found = complex_found(table, complex_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    l_dist = []\n",
    "    for i in range(len(complex_table_found)):\n",
    "        l_sub = complex_table_found['Subunit_Found'][i].split(';')\n",
    "        sub_matrix = dist_matrix.loc[l_sub, l_sub]\n",
    "        avg_dist = np.nanmean(sub_matrix.replace(0, np.nan))\n",
    "        l_dist.append(avg_dist)\n",
    "    complex_table_found['Avg_Dist'] = l_dist\n",
    "    complex_table_found['Avg_Dist_Derived'] = 1 / (1+np.array(l_dist))\n",
    "    return complex_table_found\n",
    "\n",
    "\n",
    "# Calculation of TPCA characteristics of complexes using normal distribution\n",
    "# np.nanmean: Calculate the mean and standard deviation of all protein pair distances\n",
    "# A normal distribution was constructed from the calculated mean and standard deviation, with the mean being the mean 'mu' of \n",
    "# the distances of all protein pairs and the standard deviation was the standard deviation 'sig' divide square root n of \n",
    "# the distances of all protein pairs, n was the size of the complex to be tested\n",
    "# The p-value is derived from the z-score\n",
    "def second_complex_signature_normal(table, complex_table, ref_col=1, method='cityblock'):\n",
    "    complex_table_found = complex_dist(table, complex_table, ref_col)\n",
    "    dist_matrix = dist(table, ref_col, method)\n",
    "    mu, sig = np.nanmean(dist_matrix.replace(0, np.nan)), np.nanstd(dist_matrix.replace(0, np.nan))\n",
    "    l_sig_z, l_sig_p = [], []\n",
    "    for i in range(len(complex_table_found)):\n",
    "        n = complex_table_found['No_Subunit_Found'][i]\n",
    "        avg_dist = complex_table_found['Avg_Dist'][i]\n",
    "        z = (avg_dist - mu) * (n ** (1/2)) / sig \n",
    "        l_sig_z.append(-1 * z)\n",
    "        l_sig_p.append(1 - norm.sf(z))\n",
    "    complex_table_found['TPCA_Sig_P-value'] = l_sig_p\n",
    "    complex_table_found['TPCA_Sig_Z-score'] = l_sig_z\n",
    "    return complex_table_found\n",
    "\n",
    "\n",
    "def dynamic_complex_absolute_sample(table_1, table_2, complex_table, ref_col=1, method='cityblock'):\n",
    "    table_1_align, table_2_align = align(table_1, table_2, ref_col)\n",
    "    complex_dist_1 = complex_dist(table_1_align, complex_table, ref_col, method)\n",
    "    complex_dist_2 = complex_dist(table_2_align, complex_table, ref_col, method)\n",
    "    complex_dist_change = pd.merge(complex_dist_1, complex_dist_2, on=list(complex_dist_1.columns[:-2]), suffixes=('_1','_2'))\n",
    "    complex_dist_change['Avg_Dist_change'] = complex_dist_change['Avg_Dist_1'] - complex_dist_change['Avg_Dist_2']\n",
    "    complex_dist_change['Avg_Dist_Derived_change'] = complex_dist_change['Avg_Dist_Derived_1'] - complex_dist_change['Avg_Dist_Derived_2']\n",
    "    dic_random_1 = random_10k(table_1_align, complex_table, ref_col, method)\n",
    "    dic_random_2 = random_10k(table_2_align, complex_table, ref_col, method)\n",
    "    random_table = pd.DataFrame(dic_random_1) - pd.DataFrame(dic_random_2)\n",
    "    random_table_derived = 1/(1+pd.DataFrame(dic_random_1)) - 1/(1+pd.DataFrame(dic_random_2))\n",
    "    p_value, z_score = [], []\n",
    "    for i in range(len(complex_dist_change)):\n",
    "        n = complex_dist_change['No_Subunit_Found'][i]\n",
    "        avg_dist = complex_dist_change['Avg_Dist_change'][i]\n",
    "        avg_dist_derived = complex_dist_change['Avg_Dist_Derived_change'][i]\n",
    "        l_random10000 = random_table[n]\n",
    "        l_random10000_derived = list(random_table_derived[n]) + [avg_dist_derived]      \n",
    "        p_value.append(np.sum(np.array(l_random10000)>avg_dist)/10000)\n",
    "        z_score.append(-1 * stats.zscore(l_random10000_derived)[-1])\n",
    "    complex_dist_change['Dynamic_P'] = p_value\n",
    "    complex_dist_change['Dynamic_Z'] = z_score\n",
    "    return random_table, random_table_derived, complex_dist_change\n",
    "\n",
    "\n",
    "def dynamic_complex_absolute_normal(table_1, table_2, complex_table, ref_col=1, method='cityblock'):\n",
    "    table_1_align, table_2_align = align(table_1, table_2, ref_col)\n",
    "    complex_dist_1 = complex_dist(table_1_align, complex_table, ref_col, method)\n",
    "    complex_dist_2 = complex_dist(table_2_align, complex_table, ref_col, method)\n",
    "    complex_dist_change = pd.merge(complex_dist_1, complex_dist_2, on=list(complex_dist_1.columns[:-2]), suffixes=('_1','_2'))\n",
    "    complex_dist_change['Avg_Dist_change'] = complex_dist_change['Avg_Dist_1'] - complex_dist_change['Avg_Dist_2']\n",
    "    complex_dist_change['Avg_Dist_Derived_change'] = complex_dist_change['Avg_Dist_Derived_1'] - complex_dist_change['Avg_Dist_Derived_2']\n",
    "    dist_matrix_1 = dist(table_1_align, ref_col)\n",
    "    dist_matrix_2 = dist(table_2_align, ref_col)\n",
    "    dist_matrix_dynamic = dist_matrix_1 - dist_matrix_2\n",
    "    mu, sig = np.nanmean(dist_matrix_dynamic.replace(0, np.nan)), np.nanstd(dist_matrix_dynamic.replace(0, np.nan))\n",
    "    dynamic_p = []\n",
    "    dynamic_z = []\n",
    "    for i in range(len(complex_dist_change)):\n",
    "        n = complex_dist_change['No_Subunit_Found'][i]\n",
    "        avg_dist_change = complex_dist_change['Avg_Dist_change'][i]\n",
    "        z = (avg_dist_change - mu) * (n ** (1/2)) / sig\n",
    "        dynamic_z.append(z)\n",
    "        dynamic_p.append(norm.sf(z))\n",
    "    complex_dist_change['Dynamic_Z'] = dynamic_z\n",
    "    complex_dist_change['Dynamic_P'] = dynamic_p\n",
    "    return complex_dist_change\n",
    "\n",
    "\n",
    "def random_n(table, complex_table, ref_col=1, method='cityblock', samplesize=10000):\n",
    "    l_n = list(set(complex_found(table, complex_table, ref_col)['No_Subunit_Found'])) \n",
    "    l_n.sort()\n",
    "    dic_out = {}\n",
    "    pairs_dist_table = dist(table, ref_col, method)\n",
    "    pairs_dist_table = pairs_dist_table.replace(0, np.nan);cycle_number=0\n",
    "    for num in l_n:\n",
    "        l_n_dist = [];print(len(l_n));print(cycle_number);cycle_number+=1\n",
    "        random.seed(42)\n",
    "        if num > len(list(pairs_dist_table.index)) :\n",
    "            num = len(list(pairs_dist_table.index))\n",
    "        for i in range(samplesize):\n",
    "            random_proteins = random.sample(list(pairs_dist_table.index), num)\n",
    "            random_sub_table = pairs_dist_table.loc[random_proteins, random_proteins]\n",
    "            l_n_dist.append(np.nanmean(random_sub_table))\n",
    "        dic_out[num] = l_n_dist\n",
    "    return dic_out\n",
    "\n",
    "\n",
    "def dynamic_complex_random_table_otain(table_1, table_2, complex_table, ref_col=1, method='cityblock', samplesize=10000):\n",
    "    table_1_align, table_2_align = align(table_1, table_2, ref_col)\n",
    "    complex_dist_1 = complex_dist(table_1_align, complex_table, ref_col, method)\n",
    "    complex_dist_2 = complex_dist(table_2_align, complex_table, ref_col, method)\n",
    "    complex_dist_change = pd.merge(complex_dist_1, complex_dist_2, on=list(complex_dist_1.columns[:-2]), suffixes=('_1','_2'))\n",
    "    complex_dist_change['Avg_Dist_relative_change'] = (complex_dist_change['Avg_Dist_1'] - complex_dist_change['Avg_Dist_2']) / complex_dist_change['Avg_Dist_1']\n",
    "    dic_random_1 = random_n(table_1_align, complex_table, ref_col, method, samplesize)\n",
    "    dic_random_2 = random_n(table_2_align, complex_table, ref_col, method, samplesize)\n",
    "    random_table = (pd.DataFrame(dic_random_1) - pd.DataFrame(dic_random_2)) / pd.DataFrame(dic_random_1) \n",
    "    return random_table\n",
    "\n",
    "\n",
    "# function: Switching paths between systems\n",
    "def file_path_switch_between_sys(file_path) :\n",
    "    if(platform.system()==\"Windows\") :\n",
    "        file_path=file_path.replace(\"/\", \"\\\\\")\n",
    "        return file_path\n",
    "    if(platform.system()==\"Linux\") :\n",
    "        print(\"start transf linux2\")\n",
    "        print(file_path)\n",
    "        file_path=file_path.replace(\"\\\\\", \"/\").replace(\"//\",\"/\")\n",
    "        print(file_path)\n",
    "        return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e96cd37-5cbb-4228-8029-9d8779a8e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------catalog----------\n",
    "# Because the paper has not yet been published, the data in source_data(source_catalog) has not \n",
    "# yet been uploaded,if you need it, \n",
    "# you can contact the authors of the paper or contact the authors of the repository on github to get it.\n",
    "article_catalog='article_data\\\\'\n",
    "source_catalog='source_data\\\\'\n",
    "database_catalog='database_data\\\\'\n",
    "# ----------catalog----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a10bc6-ee42-4b5c-81f0-a32a77faf072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1485\n",
      "983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------mito information creat----------\n",
    "# MitoCarta_pro_list is come from https://personal.broadinstitute.org/scalvo/MitoCarta3.0/human.mitocarta3.0.html\n",
    "MitoCarta_pro_list=pd.read_excel(file_path_switch_between_sys(database_catalog+\"MitoCarta_3_0\\\\Human.MitoCarta3.0.xls\"), sheet_name=1, header=0)['UniProt'].tolist()\n",
    "# select mito protein list, mito_SL protein list\n",
    "GOCC_mito_table=pd.read_excel(file_path_switch_between_sys(source_catalog+\"20230724_Mito_Multi.xlsx\"), sheet_name=1, header=0)\n",
    "GOCC_mito_list=GOCC_mito_table['Entry'].tolist()\n",
    "GOCC_MLP_infor_list=GOCC_mito_table[GOCC_mito_table['Multi']==1]['Entry'].tolist()\n",
    "mito_prot_list=list(set(GOCC_mito_list).union(set(MitoCarta_pro_list)))\n",
    "MLP_infor_accession_list=copy.deepcopy(GOCC_MLP_infor_list)\n",
    "mito_SL_protein_list=list(set(mito_prot_list).difference(set(MLP_infor_accession_list)))\n",
    "print(len(mito_prot_list))\n",
    "print(len(mito_SL_protein_list))\n",
    "mito_prot_list_table=pd.DataFrame(mito_prot_list, columns=['whole mito'])\n",
    "mito_SL_protein_list_table=pd.DataFrame(mito_SL_protein_list, columns=['SL mito'])\n",
    "# ----------mito information creat----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d0cf10-ec0c-4c49-8f45-4d464bd00500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene_mito_table (805, 2)\n",
      "Gene_mito_table convert to Gene_to_pro_mito_table (12, 2)\n",
      "new_mito_PPI (240, 3)\n"
     ]
    }
   ],
   "source": [
    "# ----------new mito PPI data creat----------\n",
    "\n",
    "review_table = pd.read_csv(file_path_switch_between_sys(database_catalog+'UniProt\\\\uniprot_human_string_review_2022.10.15-06.30.16.31.tsv'), sep='\\t', header=0)\n",
    "unreview_table = pd.read_csv(file_path_switch_between_sys(database_catalog+'UniProt\\\\uniprot_human_string_unreview-2022.10.15-06.29.27.31.tsv'), sep='\\t', header=0)\n",
    "uniprot_id_to_gene_id_table=pd.concat([review_table, unreview_table], axis=0, ignore_index=True)\n",
    "\n",
    "APEX_CXMS_mito_ppi = pd.read_excel(file_path_switch_between_sys(article_catalog+'mito_ppi\\\\ac2c02116_si_002.xlsx'), sheet_name='inter-links of mito APEX-CXMS')\n",
    "APEX_CXMS_mito_ppi = APEX_CXMS_mito_ppi[['Gene name1 first','Gene name2 first','itocho in GO1(1 means the mito-located protein, 0 means the non-mito located protein)']]\n",
    "APEX_CXMS_mito_ppi.columns=['Gene name1 first','Gene name2 first','itocho in GO1']\n",
    "APEX_CXMS_mito_ppi=APEX_CXMS_mito_ppi[['Gene name1 first','Gene name2 first']]\n",
    "APEX_CXMS_mito_ppi.columns=['Gene A','Gene B']\n",
    "\n",
    "CD_MS_mito_ppi = pd.read_excel(file_path_switch_between_sys(article_catalog+'mito_ppi\\\\41467_2023_39485_MOESM4_ESM.xlsx'), sheet_name='Figure 3a-3b', skiprows=1, header=0, usecols=[0,1,2,4])\n",
    "CD_MS_mito_ppi = CD_MS_mito_ppi.iloc[0:152,:][['protein1','protein2']]\n",
    "CD_MS_mito_ppi.columns=['Gene A','Gene B']\n",
    "\n",
    "Sub_CL_mito_ppi = pd.read_excel(file_path_switch_between_sys(article_catalog+'mito_ppi\\\\ac2c01637_si_002.xlsx'), sheet_name='Table S10.', skiprows=2, header=0, usecols=[14,15,16,17,18,19]).dropna()\n",
    "Sub_CL_mito_ppi.columns=['Protein A', 'Protein B', 'From site', 'To site', 'Pro a is_mito', 'Pro b is_mito']\n",
    "SubPiXL_mito_ppi = pd.read_excel(file_path_switch_between_sys(article_catalog+'mito_ppi\\\\ac2c01637_si_002.xlsx'), sheet_name='Table S10.', skiprows=2, header=0, usecols=[21,22,23,24,25,26]).dropna()\n",
    "SubPiXL_mito_ppi.columns=['Protein A', 'Protein B', 'From site', 'To site', 'Pro a is_mito', 'Pro b is_mito']\n",
    "SubPiXL_mito_table=pd.concat([Sub_CL_mito_ppi,SubPiXL_mito_ppi],axis=0)[['Protein A', 'Protein B']]\n",
    "SubPiXL_mito_table=pd_col_exchange_for_merge(SubPiXL_mito_table,'Protein A','Protein B')\n",
    "SubPiXL_mito_table['align']=SubPiXL_mito_table['Protein A']+'_vs_'+SubPiXL_mito_table['Protein B']\n",
    "SubPiXL_mito_table=SubPiXL_mito_table.drop_duplicates(subset=['align'],keep='first')\n",
    "SubPiXL_mito_table=SubPiXL_mito_table[['Protein A','Protein B']]\n",
    "\n",
    "Gene_mito_table=pd.concat([APEX_CXMS_mito_ppi, CD_MS_mito_ppi],axis=0)\n",
    "print('Gene_mito_table',Gene_mito_table.shape)\n",
    "\n",
    "geneID_to_proteinID_table=uniprot_id_to_gene_id_table[uniprot_id_to_gene_id_table['Entry'].isin(mito_prot_list)]\n",
    "geneID_to_proteinID_table=geneID_to_proteinID_table[['Entry','Gene Names']].drop_duplicates(subset=['Entry'],keep='first')\n",
    "geneID_to_proteinID_table=geneID_to_proteinID_table[['Entry','Gene Names']].drop_duplicates(subset=['Gene Names'],keep='first')\n",
    "geneID_to_proteinID_table.columns=['Protein A','Gene A']\n",
    "Gene_to_pro_mito_table=pd.merge(Gene_mito_table,geneID_to_proteinID_table,how='left',on='Gene A').dropna()\n",
    "geneID_to_proteinID_table.columns=['Protein B','Gene B']\n",
    "Gene_to_pro_mito_table=pd.merge(Gene_to_pro_mito_table,geneID_to_proteinID_table,how='left',on='Gene B').dropna()\n",
    "Gene_to_pro_mito_table=Gene_to_pro_mito_table[['Protein A','Protein B']]\n",
    "print('Gene_mito_table convert to Gene_to_pro_mito_table',Gene_to_pro_mito_table.shape)\n",
    "\n",
    "pro_mito_table=pd.concat([SubPiXL_mito_table],axis=0)\n",
    "whole_pro_mito_table=pd.concat([Gene_to_pro_mito_table,pro_mito_table],axis=0)\n",
    "whole_pro_mito_table=pd_col_exchange_for_merge(whole_pro_mito_table,'Protein A','Protein B')\n",
    "\n",
    "whole_pro_mito_table['No. of Publications']=[1]*len(whole_pro_mito_table)\n",
    "whole_pro_mito_table=whole_pro_mito_table[['Protein A','Protein B','No. of Publications']]\n",
    "new_mito_PPI=whole_pro_mito_table[whole_pro_mito_table['Protein A']!=whole_pro_mito_table['Protein B']]\n",
    "print('new_mito_PPI',new_mito_PPI.shape)\n",
    "# ----------new mito PPI data creat----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
